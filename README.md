# Building a Data Processing Pipeline Using Spark
## project descripe
In this project, we aim to improve the data processing pipeline (code available on github -
https://github.com/VIDA-NYU/wildlife_pipeline) using Apache Spark. The pipeline involves tasks
such as information extraction, data filtering, and text inference. The goal is to obtain structured
data that can be used for further analysis.
Goal: The main objective of this project is to demonstrate how Spark can be used to process
large volumes of data efficiently, particularly in the context of web scraping and text
classification tasks, and at the end show the comparison of performances.

## Team
- Tianfan Yang
- Dhruv Nain
- Shuhao Ruan

## intro
This projects focuses on leveraging big data processing technologies, specifically Apache Spark, to improve the data processing pipeline for wildlife trade. Our research goal is utilize these technology to efficiently extract, filter, and classify advertising data, get structured data sets, support future analysis and research. We want to show how to use Spark to process large-scale data in web scraping and text classification applications, and show the comparison of performances. 

